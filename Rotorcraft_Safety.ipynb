{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training on 17707 sample, validating on 932 samples\n",
    "Number of features: 2285\n",
    "Training accuracy with kNN: 0.9435816343818829\n",
    "Validation accuracy with kNN: 0.9334763948497854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training on 17707 sample, validating on 932 samples\n",
    "Number of features: 2285\n",
    "/home/khanhi83/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
    "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
    "Training accuracy with Random Forest: 0.9952561134014797\n",
    "Validation accuracy with Random Forest: 0.9281115879828327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training on 17707 sample, validating on 932 samples\n",
    "Number of features: 2285\n",
    "Training accuracy with MLP: 0.9875755350991133\n",
    "Validation accuracy with MLP: 0.9345493562231759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . /home/khanhi83/.local/share/virtualenvs/Rotorcraft-Safety-BOPmdVWD/bin/activate\n",
    "# To activate this project's virtualenv, run pipenv shell.\n",
    "# Alternatively, run a command inside the virtualenv with pipenv run.\n",
    "# import structlog\n",
    "# !which python\n",
    "# !conda install -c conda-forge structlog\n",
    "# !pip install typing_extensions\n",
    "# import structlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/khanhi83/anaconda3/envs/PY3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP: 1.18.1\n",
      "PD: 1.0.1\n",
      "Request: 2.22.0\n",
      "SKLearn: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# For mutliple devices (GPUs: 4, 5, 6, 7)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "print(\"NP:\", np.__version__)\n",
    "print(\"PD:\", pd.__version__)\n",
    "print(\"Request:\", requests.__version__)\n",
    "print(\"SKLearn:\", sklearn.__version__)\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (16.0, 12.0)})\n",
    "HEADER_COLUMN = 12\n",
    "LABEL_COLUMN = 'False Warning'\n",
    "TEXT_COLUMN = 'Text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, local_dir: str = '.', local_filename: str = '') -> str:\n",
    "    \"\"\"\n",
    "    Downloads a file from a provided url to a local directory\n",
    "    :param url: URL to download the file from\n",
    "    :param local_dir: Local directory to download the file to (created if it does not exist)\n",
    "    :param local_filename: What to name the file when saved\n",
    "     (if empty or none, assume the name of the original name of the file)\n",
    "    :return: the name of the file which was saved\n",
    "    \"\"\"\n",
    "    os.makedirs(f'{local_dir}', exist_ok=True)\n",
    "    local_filename = local_filename if local_filename else url.split('/')[-1]\n",
    "    if os.path.exists(f'{local_dir}/{local_filename}'):\n",
    "        print(\"{0}/{1} already exists. Skipping download.\".format(local_dir, local_filename))\n",
    "    else:\n",
    "        print(\"Downloading file from {0} to {1}/{2}.\".format(url, local_dir, local_filename))\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(f'./{local_dir}/{local_filename}', 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=128):\n",
    "                    f.write(chunk)\n",
    "        print(\"Finished saving file from {0} to {1}/{2}.\".format(url, local_dir, local_filename))\n",
    "    return f'{local_dir}/{local_filename}'\n",
    "\n",
    "\n",
    "def unzip_file(path_to_zip_file: str, dir_to_extract_to: str) -> str:\n",
    "    \"\"\"\n",
    "    Unzips a zip file to a provided directory\n",
    "    :param path_to_file: path to zip file\n",
    "    :param dir_to_extract_to: directory to extract zip file\n",
    "    :return: full path to unzipped file (assuming there is only one)\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dir_to_extract_to)\n",
    "        return f'{dir_to_extract_to}/{zip_ref.namelist()[0]}'\n",
    "\n",
    "\n",
    "def load_data(path_to_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads excel data from a supplied path into a Pandas dataframe\n",
    "    :param path_to_file: path to excel file\n",
    "    :return: Pandas dataframe containing contents of excel spreadsheet\n",
    "    \"\"\"\n",
    "    print(\"Started loading the excel data from {0} into a datafram - this may take a while. You may want to grab a coffee.\".format(path_to_file))\n",
    "    df = pd.read_excel(path_to_file, engine='openpyxl', header=HEADER_COLUMN)\n",
    "    print(\"Finished loading the excel data from {0} into a dataframe.\".format(path_to_file))\n",
    "    return df\n",
    "\n",
    "\n",
    "def vectorize(df: pd.DataFrame, **kwargs) -> Tuple[np.array, List[str]]:\n",
    "    print(\"Converting text to feature matrix\")\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    sparse_matrix = vectorizer.fit_transform(df[TEXT_COLUMN])\n",
    "    feature_matrix = sparse_matrix.todense()\n",
    "    return feature_matrix, vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "def extract_and_encode_labels(df: pd.DataFrame) -> Tuple[np.array, Dict[str, int]]:\n",
    "    label_mapping = dict((label, i) for i, label in enumerate(df[LABEL_COLUMN].unique()))\n",
    "    labels = list(df[LABEL_COLUMN].map(label_mapping))\n",
    "    return np.array(labels), label_mapping\n",
    "\n",
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khanhi83/anaconda3/envs/PY3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2,5,9,12,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,87,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,117,118,119,121,122,123,124,125,126,127,128,130,131,132,145,146,148,215,224) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 3198 records because Text was null or NaN\n",
      "Dropped 4438 records because False Warning was null or NaN\n",
      "Converting text to feature matrix\n",
      "Training on 17707 sample, validating on 932 samples\n",
      "Number of features: 2285\n",
      "MLP training: This may take a while. You may want to grab a coffee.\n",
      "Iteration 1, loss = 1.20353836\n",
      "Iteration 2, loss = 0.37653667\n",
      "Iteration 3, loss = 0.28721284\n",
      "Iteration 4, loss = 0.24405227\n",
      "Iteration 5, loss = 0.20639902\n",
      "Iteration 6, loss = 0.16921061\n",
      "Iteration 7, loss = 0.14159178\n",
      "Iteration 8, loss = 0.12019215\n",
      "Iteration 9, loss = 0.10609428\n",
      "Iteration 10, loss = 0.09564276\n",
      "Iteration 11, loss = 0.08727815\n",
      "Iteration 12, loss = 0.08054414\n",
      "Iteration 13, loss = 0.07615574\n",
      "Iteration 14, loss = 0.07225505\n",
      "Iteration 15, loss = 0.06736530\n",
      "Iteration 16, loss = 0.06054300\n",
      "Iteration 17, loss = 0.05184826\n",
      "Iteration 18, loss = 0.04625869\n",
      "Iteration 19, loss = 0.03959450\n",
      "Iteration 20, loss = 0.03449475\n",
      "Iteration 21, loss = 0.03045931\n",
      "Iteration 22, loss = 0.02841904\n",
      "Iteration 23, loss = 0.02720997\n",
      "Iteration 24, loss = 0.02742023\n",
      "Iteration 25, loss = 0.02882297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khanhi83/anaconda3/envs/PY3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy with MLP: 0.9875755350991133\n",
      "Validation accuracy with MLP: 0.9345493562231759\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    local_dir = './data'\n",
    "\n",
    "    compute_features = not os.path.exists(f'{local_dir}/feature_data.csv')\n",
    "    model_type = \"mlp\" #\"{knn\", \"mlp\", \"rf\"}  \n",
    "\n",
    "    if compute_features:\n",
    "        # download the file\n",
    "        path_to_downloaded_zip_file = download_file(\n",
    "            'https://www.fire.tc.faa.gov/zip/MasterModelVersion3DDeliverable.zip',\n",
    "            local_dir)\n",
    "        # unzip the file\n",
    "        path_to_file = unzip_file(path_to_downloaded_zip_file, local_dir)\n",
    "\n",
    "        # load the file into a Pandas dataframe\n",
    "        df = load_data(path_to_file)\n",
    "\n",
    "        # save preprocessed data to save time for future runs\n",
    "        df.to_csv(f'{local_dir}/feature_data.csv')\n",
    "    else:\n",
    "        # don't go through the hassle of preprocessing if we already have the preprocessed data saved\n",
    "        df = pd.read_csv(f'{local_dir}/feature_data.csv')\n",
    "\n",
    "    count_of_no_text = len(df[df[TEXT_COLUMN].isnull()])\n",
    "    df = df.dropna(subset=[TEXT_COLUMN])\n",
    "    print(\"Dropped {0} records because {1} was null or NaN\".format(count_of_no_text, TEXT_COLUMN))\n",
    "\n",
    "    count_of_null_labels = len(df[df[LABEL_COLUMN].isnull()])\n",
    "    df = df.dropna(subset=[LABEL_COLUMN])\n",
    "    print(\"Dropped {0} records because {1} was null or NaN\".format(count_of_null_labels, LABEL_COLUMN))\n",
    "\n",
    "    # create a sparse feature matrix of size n x m,\n",
    "    # where n = number of documents, m = number of words in vocabulary\n",
    "    feature_matrix, feature_names = vectorize(df, min_df=0.001)\n",
    "\n",
    "    labels, label_mapping = extract_and_encode_labels(df)\n",
    "    num_labels = len(label_mapping)\n",
    "    num_features = feature_matrix.shape[1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.05, random_state=1)\n",
    "\n",
    "    print(\"Training on {0} sample, validating on {1} samples\".format(X_train.shape[0], X_test.shape[0]))\n",
    "    print(\"Number of features: {0}\".format(num_features))\n",
    "\n",
    "    if model_type == \"mlp\":\n",
    "        print(\"MLP training: This may take a while. You may want to grab a coffee.\")\n",
    "        model = MLPClassifier(hidden_layer_sizes=(num_features, 256, 128, 64, 32, num_labels),\n",
    "                              #(8192, 2048, 512, 128, 32, 8, num_labels), \n",
    "                  activation='relu', solver='adam', verbose=1, learning_rate_init=0.01, batch_size=4096,\n",
    "                              max_iter=25)\n",
    "        model.fit(X_train, y_train)\n",
    "        predict_train = model.predict(X_train)\n",
    "        predict_test = model.predict(X_test)\n",
    "        training_acc = confusion_matrix(predict_train, y_train)\n",
    "        validation_acc = confusion_matrix(predict_test, y_test)\n",
    "        print(\"Training accuracy with MLP: {0}\".format(accuracy(training_acc)))\n",
    "        print(\"Validation accuracy with MLP: {0}\".format(accuracy(validation_acc)))\n",
    "    elif model_type == \"rf\":\n",
    "        rf = RandomForestClassifier(n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "        training_acc = rf.score(X_train, y_train)\n",
    "        validation_acc = rf.score(X_test, y_test)\n",
    "        print(\"Training accuracy with Random Forest: {0}\".format(training_acc))\n",
    "        print(\"Validation accuracy with Random Forest: {0}\".format(validation_acc))\n",
    "    elif model_type == \"knn\":\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "        knn.fit(X_train, y_train)\n",
    "        training_acc = knn.score(X_train, y_train)\n",
    "        validation_acc = knn.score(X_test, y_test)\n",
    "        print(\"Training accuracy with kNN: {0}\".format(training_acc))\n",
    "        print(\"Validation accuracy with kNN: {0}\".format(validation_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dropped 3198 records because Text was null or NaN\n",
    "Dropped 4438 records because False Warning was null or NaN\n",
    "Converting text to feature matrix\n",
    "Training on 17707 sample, validating on 932 samples\n",
    "Number of features: 2285\n",
    "MLP training: This may take a while. You may want to grab a coffee.\n",
    "Iteration 1, loss = 1.20353836\n",
    "Iteration 2, loss = 0.37653667\n",
    "Iteration 3, loss = 0.28721284\n",
    "Iteration 4, loss = 0.24405227\n",
    "Iteration 5, loss = 0.20639902\n",
    "Iteration 6, loss = 0.16921061\n",
    "Iteration 7, loss = 0.14159178\n",
    "Iteration 8, loss = 0.12019215\n",
    "Iteration 9, loss = 0.10609428\n",
    "Iteration 10, loss = 0.09564276\n",
    "Iteration 11, loss = 0.08727815\n",
    "Iteration 12, loss = 0.08054414\n",
    "Iteration 13, loss = 0.07615574\n",
    "Iteration 14, loss = 0.07225505\n",
    "Iteration 15, loss = 0.06736530\n",
    "Iteration 16, loss = 0.06054300\n",
    "Iteration 17, loss = 0.05184826\n",
    "Iteration 18, loss = 0.04625869\n",
    "Iteration 19, loss = 0.03959450\n",
    "Iteration 20, loss = 0.03449475\n",
    "Iteration 21, loss = 0.03045931\n",
    "Iteration 22, loss = 0.02841904\n",
    "Iteration 23, loss = 0.02720997\n",
    "Iteration 24, loss = 0.02742023\n",
    "Iteration 25, loss = 0.02882297\n",
    "/home/khanhi83/anaconda3/envs/PY3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
    "  % self.max_iter, ConvergenceWarning)\n",
    "Training accuracy with MLP: 0.9875755350991133\n",
    "Validation accuracy with MLP: 0.9345493562231759"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/home/khanhi83/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (2,5,9,12,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,87,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,117,118,119,121,122,123,124,125,126,127,128,130,131,132,145,146,148,215,224) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "  interactivity=interactivity, compiler=compiler, result=result)\n",
    "Dropped 3198 records because Text was null or NaN\n",
    "Dropped 4438 records because False Warning was null or NaN\n",
    "Converting text to feature matrix\n",
    "Training on 17707 sample, validating on 932 samples\n",
    "Number of features: 2285\n",
    "/home/khanhi83/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
    "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
    "Training accuracy with Random Forest: 0.9952561134014797\n",
    "Validation accuracy with Random Forest: 0.9281115879828327"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/home/khanhi83/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (2,5,9,12,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,87,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,117,118,119,121,122,123,124,125,126,127,128,130,131,132,145,146,148,215,224) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "  interactivity=interactivity, compiler=compiler, result=result)\n",
    "Dropped 3198 records because Text was null or NaN\n",
    "Dropped 4438 records because False Warning was null or NaN\n",
    "Converting text to feature matrix\n",
    "Training on 17707 sample, validating on 932 samples\n",
    "Number of features: 2285\n",
    "Training accuracy with kNN: 0.9435816343818829\n",
    "Validation accuracy with kNN: 0.9334763948497854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
