{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . /home/khanhi83/.local/share/virtualenvs/Rotorcraft-Safety-BOPmdVWD/bin/activate\n",
    "# To activate this project's virtualenv, run pipenv shell.\n",
    "# Alternatively, run a command inside the virtualenv with pipenv run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which python\n",
    "# !conda install -c conda-forge structlog\n",
    "# !pip install typing_extensions\n",
    "# import structlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# For mutliple devices (GPUs: 4, 5, 6, 7)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "# import structlog\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tensorflow import keras, one_hot\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (16.0, 12.0)})\n",
    "# _LOGGER = structlog.get_logger(__file__)\n",
    "HEADER_COLUMN = 12\n",
    "LABEL_COLUMN = 'False Warning'\n",
    "TEXT_COLUMN = 'Text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_file(url: str, local_dir: str = '.', local_filename: str = '') -> str:\n",
    "    \"\"\"\n",
    "    Downloads a file from a provided url to a local directory\n",
    "    :param url: URL to download the file from\n",
    "    :param local_dir: Local directory to download the file to (created if it does not exist)\n",
    "    :param local_filename: What to name the file when saved\n",
    "     (if empty or none, assume the name of the original name of the file)\n",
    "    :return: the name of the file which was saved\n",
    "    \"\"\"\n",
    "    os.makedirs(f'{local_dir}', exist_ok=True)\n",
    "    local_filename = local_filename if local_filename else url.split('/')[-1]\n",
    "    if os.path.exists(f'{local_dir}/{local_filename}'):\n",
    "#         _LOGGER.info(f'{local_dir}/{local_filename} already exists. Skipping download.')\n",
    "        print(\"{0}/{1} already exists. Skipping download.\".format(local_dir, local_filename))\n",
    "    else:\n",
    "#         _LOGGER.info(f\"Downloading file from {url} to {local_dir}/{local_filename}.\")\n",
    "        print(\"Downloading file from {0} to {1}/{2}.\".format(url, local_dir, local_filename))\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(f'./{local_dir}/{local_filename}', 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=128):\n",
    "                    f.write(chunk)\n",
    "#         _LOGGER.info(f\"Finished saving file from {url} to {local_dir}/{local_filename}.\")\n",
    "        print(\"Finished saving file from {0} to {1}/{2}.\".format(url, local_dir, local_filename))\n",
    "    return f'{local_dir}/{local_filename}'\n",
    "\n",
    "\n",
    "def unzip_file(path_to_zip_file: str, dir_to_extract_to: str) -> str:\n",
    "    \"\"\"\n",
    "    Unzips a zip file to a provided directory\n",
    "    :param path_to_file: path to zip file\n",
    "    :param dir_to_extract_to: directory to extract zip file\n",
    "    :return: full path to unzipped file (assuming there is only one)\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dir_to_extract_to)\n",
    "        return f'{dir_to_extract_to}/{zip_ref.namelist()[0]}'\n",
    "\n",
    "\n",
    "def load_data(path_to_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads excel data from a supplied path into a Pandas dataframe\n",
    "    :param path_to_file: path to excel file\n",
    "    :return: Pandas dataframe containing contents of excel spreadsheet\n",
    "    \"\"\"\n",
    "#     _LOGGER.info(f\"Started loading the excel data from {path_to_file} into a dataframe - this may take a while. \"\n",
    "#                  f\"You may want to grab a coffee.\")\n",
    "    print(\"Started loading the excel data from {0} into a datafram - this may take a while. You may want to grab a coffee.\".format(path_to_file))\n",
    "    df = pd.read_excel(path_to_file, engine='openpyxl', header=HEADER_COLUMN)\n",
    "#     _LOGGER.info(f\"Finished loading the excel data from {path_to_file} into a dataframe.\")\n",
    "    print(\"Finished loading the excel data from {0} into a dataframe.\".format(path_to_file))\n",
    "    return df\n",
    "\n",
    "\n",
    "def vectorize(df: pd.DataFrame, **kwargs) -> Tuple[np.array, List[str]]:\n",
    "#     _LOGGER.info(\"Converting text to feature matrix\")\n",
    "    print(\"Converting text to feature matrix\")\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    sparse_matrix = vectorizer.fit_transform(df[TEXT_COLUMN])\n",
    "    feature_matrix = sparse_matrix.todense()\n",
    "    return feature_matrix, vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "def extract_and_encode_labels(df: pd.DataFrame) -> Tuple[np.array, Dict[str, int]]:\n",
    "    label_mapping = dict((label, i) for i, label in enumerate(df[LABEL_COLUMN].unique()))\n",
    "    labels = list(df[LABEL_COLUMN].map(label_mapping))\n",
    "    return np.array(labels), label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/MasterModelVersion3DDeliverable.zip already exists. Skipping download.\n",
      "Started loading the excel data from ./data/Master Model Version 3.0D Deliverable.xlsm into a datafram - this may take a while. You may want to grab a coffee.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    local_dir = './data'\n",
    "\n",
    "    compute_features = not os.path.exists(f'{local_dir}/feature_data.csv')\n",
    "    model_type = \"mlp\" #\"{knn\", \"mlp\", \"rf\"}  \n",
    "\n",
    "    if compute_features:\n",
    "        # download the file\n",
    "        path_to_downloaded_zip_file = download_file(\n",
    "            'https://www.fire.tc.faa.gov/zip/MasterModelVersion3DDeliverable.zip',\n",
    "            local_dir)\n",
    "        # unzip the file\n",
    "        path_to_file = unzip_file(path_to_downloaded_zip_file, local_dir)\n",
    "\n",
    "        # load the file into a Pandas dataframe\n",
    "        df = load_data(path_to_file)\n",
    "\n",
    "        # save preprocessed data to save time for future runs\n",
    "        df.to_csv(f'{local_dir}/feature_data.csv')\n",
    "    else:\n",
    "        # don't go through the hassle of preprocessing if we already have the preprocessed data saved\n",
    "        df = pd.read_csv(f'{local_dir}/feature_data.csv')\n",
    "\n",
    "    count_of_no_text = len(df[df[TEXT_COLUMN].isnull()])\n",
    "    df = df.dropna(subset=[TEXT_COLUMN])\n",
    "#     _LOGGER.info(f\"Dropped {count_of_no_text} records because {TEXT_COLUMN} was null or NaN\")\n",
    "    print(\"Dropped {0} records because {1} was null or NaN\".format(count_of_no_text, TEXT_COLUMN))\n",
    "\n",
    "    count_of_null_labels = len(df[df[LABEL_COLUMN].isnull()])\n",
    "    df = df.dropna(subset=[LABEL_COLUMN])\n",
    "#     _LOGGER.info(f\"Dropped {count_of_null_labels} records because {LABEL_COLUMN} was null or NaN\")\n",
    "    print(\"Dropped {0} records because {1} was null or NaN\".format(count_of_null_labels, LABEL_COLUMN))\n",
    "\n",
    "    # create a sparse feature matrix of size n x m,\n",
    "    # where n = number of documents, m = number of words in vocabulary\n",
    "    feature_matrix, feature_names = vectorize(df, min_df=0.001)\n",
    "\n",
    "    labels, label_mapping = extract_and_encode_labels(df)\n",
    "    num_labels = len(label_mapping)\n",
    "    num_features = feature_matrix.shape[1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.05, random_state=1)\n",
    "\n",
    "#     _LOGGER.info(f\"Training on {X_train.shape[0]} samples, validating on {X_test.shape[0]} samples.\")\n",
    "    print(\"Training on {0} sample, validating on {1} samples\".format(X_train.shape[0], X_test.shape[0]))\n",
    "#     _LOGGER.info(f\"Number of features: {num_features}\")\n",
    "    print(\"Number of features: {0}\".format(num_features))\n",
    "\n",
    "    if model_type == \"mlp\":\n",
    "        labels = one_hot(np.array(labels), len(label_mapping))\n",
    "        inputs = keras.Input(shape=(num_features,))\n",
    "        layer_1 = layers.Dense(8192, activation=ReLU())(inputs)\n",
    "        layer_2 = layers.Dense(2048, activation=ReLU())(layer_1)\n",
    "        layer_3 = layers.Dense(512, activation=ReLU())(layer_2)\n",
    "        layer_4 = layers.Dense(128, activation=ReLU())(layer_3)\n",
    "        layer_5 = layers.Dense(32, activation=ReLU())(layer_4)\n",
    "        layer_6 = layers.Dense(8, activation=ReLU())(layer_5)\n",
    "        outputs = layers.Dense(num_labels, activation=\"softmax\")(layer_6)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#         _LOGGER.info(model.summary())\n",
    "        print(model.summary())\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adamax(),  # Optimizer\n",
    "            loss=keras.losses.CategoricalCrossentropy(),  # Loss function to minimize\n",
    "            metrics=[keras.metrics.Accuracy()]  # List of metrics to monitor\n",
    "        )\n",
    "#         print(\"X_Train:\", X_train.shape, \" y_Train:\", y_train.shape)\n",
    "        #Changed\n",
    "        y_train = to_categorical(y_train)\n",
    "        y_test = to_categorical(y_test)\n",
    "        #Changed\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test), shuffle=True, epochs=500, batch_size=128,\n",
    "                  callbacks=[CSVLogger('./results.csv')])\n",
    "        model.save('model')\n",
    "    elif model_type == \"rf\":\n",
    "        rf = RandomForestClassifier(n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "        training_acc = rf.score(X_train, y_train)\n",
    "        validation_acc = rf.score(X_test, y_test)\n",
    "#         _LOGGER.info(f\"Training accuracy with Random Forest: {training_acc}\")\n",
    "        print(\"Training accuracy with Random Forest: {0}\".format(training_acc))\n",
    "#         _LOGGER.info(f\"Validation accuracy with Random Forest: {validation_acc}\")\n",
    "        print(\"Validation accuracy with Random Forest: {0}\".format(validation_acc))\n",
    "    elif model_type == \"knn\":\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "        knn.fit(X_train, y_train)\n",
    "        training_acc = knn.score(X_train, y_train)\n",
    "        validation_acc = knn.score(X_test, y_test)\n",
    "#         _LOGGER.info(f\"Training accuracy with kNN: {training_acc}\")\n",
    "        print(\"Training accuracy with kNN: {0}\".format(training_acc))\n",
    "#         _LOGGER.info(f\"Validation accuracy with kNN: {validation_acc}\")\n",
    "        print(\"Validation accuracy with kNN: {0}\".format(validation_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
